1. Дайте визначення поняттям: модель, математична модель.  
Модель — це спрощене (ідеалізоване) відображення об’єкта/процесу, яке зберігає суттєві властивості для заданої мети. Математична модель — це опис об’єкта/процесу мовою математики (змінні, параметри, рівняння, нерівності, ймовірності), що дозволяє виконувати розрахунки й прогноз.

2. Класифікація математичних моделей, їх особливості.  
Класифікують за: детерміновані/стохастичні; статичні/динамічні; дискретні/неперервні; лінійні/нелінійні; зосереджених параметрів/розподілених; аналітичні/імітаційні/емпіричні. Особливість класу визначає спосіб аналізу (напр., для стохастичних потрібні ймовірнісні характеристики, для динамічних — еволюція в часі).

3. Які переваги математичної моделі. Поняття алгоритм, програма  
Переваги: дає можливість прогнозувати, оптимізувати, порівнювати сценарії, зменшувати витрати експериментів, формалізувати знання й перевіряти гіпотези. Алгоритм — скінченна, однозначна послідовність кроків для розв’язання задачі; програма — реалізація алгоритму мовою програмування для виконання на ЕОМ.

4. Перерахуйте сучасні методи оптимізації.  
Градієнтні методи (GD/SGD, momentum, Adam), методи другого порядку (Newton, квазі-Ньютон L-BFGS), внутрішньоточкові методи, координатний спуск, проксимальні/ADMM, стохастична та робастна оптимізація, баєсівська оптимізація, еволюційні та метаевристики (GA, PSO, simulated annealing, tabu).

5. Перерахуйте класичні методи оптимізації.  
Аналітичні (умови оптимуму, похідні), множники Лагранжа, методи градієнтного спуску, Ньютона, спряжених градієнтів, симплекс-метод (ЛП), динамічне програмування, методи випадкового пошуку/перебору для малих розмірностей.

6. Дайте визначення поняттям: цільова функція, критерій оптимізації.
Цільова функція — функція (f(x)), значення якої мінімізують/максимізують. Критерій оптимізації — правило/показник, за яким порівнюють допустимі рішення і визначають «краще» (часто це й є цільова функція або набір цілей).

7. Види критеріїв оптимізації
Однокритеріальні й багатокритеріальні (векторні); детерміновані й стохастичні (за матсподіванням, ризиком); абсолютні/відносні; мінімізація/максимізація; техніко-економічні (вартість, час, якість, надійність, прибуток).

8. Перерахуйте основні етапи оптимізаційного моделювання.
Постановка задачі → вибір змінних/параметрів → формування цілі та обмежень → вибір методу/алгоритму → розв’язання → перевірка адекватності/чутливості → інтерпретація та впровадження.

9. Послідовність підготовки і вирішення задач на ЕОМ.
Аналіз задачі → формалізація (модель) → розробка алгоритму → програмування → налагодження/тестування → обчислювальний експеримент → аналіз результатів → документація.

10. Призначення кореляційно - регресійного аналізу.
Кореляційний аналіз вимірює силу/напрям статистичного зв’язку, регресійний — будує рівняння залежності та оцінює вплив факторів для пояснення й прогнозування.

11. Кореляційні і функціональні залежності.
Функціональна залежність — однозначний зв’язок (y=f(x)) (без випадковості). Кореляційна — статистичний зв’язок із розсіюванням значень (вплив випадкових чинників).

12. Коефіцієнт кореляції та його граничні значення.
Коефіцієнт кореляції Пірсона (r) лежить у межах ([-1;1]): (r=1) — ідеальна пряма, (r=-1) — ідеальна обернена, (r\approx 0) — лінійного зв’язку майже немає.

13. Формули обчислення парної і множинної коефіцієнтів кореляції.
Парна: (r_{xy}=\dfrac{\mathrm{cov}(x,y)}{\sigma_x\sigma_y}=\dfrac{\sum (x_i-\bar x)(y_i-\bar y)}{\sqrt{\sum (x_i-\bar x)^2\sum (y_i-\bar y)^2}}). Множинна: (R=\sqrt{R^2}), де (R^2=1-\dfrac{\sum (y_i-\hat y_i)^2}{\sum (y_i-\bar y)^2}) (частка поясненої дисперсії).

14. Оцінка точності апроксимації нелінійної залежності.
Оцінюють за похибками/залишками: SSE, MSE/RMSE, MAE/MAPE, (R^2), аналізом залишків (випадковість, відсутність тренду), а також критеріями AIC/BIC для порівняння моделей.

15. Суть методу найменших квадратів.
Параметри моделі підбирають так, щоб мінімізувати суму квадратів відхилень: (\min \sum_{i}(y_i-\hat y_i)^2).

16. Види парних регресій.
Лінійна, поліноміальна (квадратична тощо), степенева, показникова, логарифмічна, гіперболічна та інші задані аналітичні форми (y=f(x)).

17. Способи лінеаризації нелінійних залежностей.
Заміни змінних/перетворення: (\ln y), (\ln x), (1/x), (1/y), (y^a) тощо, щоб звести до вигляду (Y=A+B X) і застосувати МНК.

18. Множинна лінійна регресія.
Модель: (y=\beta_0+\beta_1x_1+\dots+\beta_kx_k+\varepsilon); параметри оцінюють МНК, інтерпретують як граничний вплив факторів за фіксації інших.

19. Послідовність обчислення параметрів множинної регресії методом Крамера.
Складають нормальні рівняння (A\beta=b) (де (A=X^\top X), (b=X^\top y)); обчислюють (\Delta=\det(A)); для кожного (\beta_j) замінюють (j)-й стовпець (A) на (b) і беруть (\Delta_j); тоді (\beta_j=\Delta_j/\Delta).

20. Послідовність обчислення параметрів множинної регресії матричним способом.
Формують матрицю ознак (X) (із стовпцем одиниць) і вектор (y); далі (\hat\beta=(X^\top X)^{-1}X^\top y) (за умови оборотності (X^\top X)).

21. Дайте визначення дискретної і неперервної випадкової величини.
Дискретна ВВ приймає скінченну/зліченну множину значень (має ймовірності (P(X=x))). Неперервна ВВ приймає значення на інтервалі (описується щільністю (f(x)), і (P(a<X<b)=\int_a^b f(x),dx)).

22. Перерахуйте основні характеристики випадкових величин.
Функція розподілу; для дискретної — закон (PMF), для неперервної — щільність (PDF); математичне сподівання, дисперсія, СКВ, моменти, медіана, мода, квантили, асиметрія, ексцес.

23. Особливості біноміального закону розподілу.
Описує число успіхів у (n) незалежних випробуваннях Бернуллі з імовірністю успіху (p): (P(X=k)=\binom{n}{k}p^k(1-p)^{n-k}); (E=np), (D=np(1-p)).

24. Особливості закону Пуассона.
Описує кількість подій за інтервал при середньому (\lambda): (P(X=k)=e^{-\lambda}\lambda^k/k!); (E=D=\lambda); часто наближає біноміальний при великому (n), малому (p), (\lambda=np).

25. Особливості нормального закону розподілу.
Симетричний «дзвін», параметри (\mu,\sigma^2); щільність (\frac{1}{\sigma\sqrt{2\pi}}e^{-(x-\mu)^2/(2\sigma^2)}); ключовий через ЦГТ (суми багатьох малих незалежних впливів).

26. Особливості закону рівномірної щільності.
На ([a,b]): (f(x)=1/(b-a)); (E=(a+b)/2), (D=(b-a)^2/12); усі значення в інтервалі «однаково ймовірні» за щільністю.

27. Особливості показового закону розподілу.
(f(t)=\lambda e^{-\lambda t}) для (t\ge0); властивість безпам’ятності; (E=1/\lambda), (D=1/\lambda^2); типовий для часу між подіями пуассонівського потоку.

28. Особливості закону Вейбулла.
Гнучкий для надійності/відмов: (f(t)=\frac{k}{\lambda}(t/\lambda)^{k-1}e^{-(t/\lambda)^k}); при (k<1) інтенсивність спадає, (k=1) — експоненційний, (k>1) — зростає.

29. Дайте визначення генеральної і вибіркової сукупності.
Генеральна сукупність — всі об’єкти/спостереження, що цікавлять. Вибірка — підмножина генеральної сукупності, відібрана для оцінювання її характеристик.

30. Основні характеристики вибіркової сукупності і способи їх обчислення.
(\bar x=\frac1n\sum x_i); вибіркова дисперсія (незміщена) (s^2=\frac1{n-1}\sum (x_i-\bar x)^2); (s=\sqrt{s^2}); медіана/квантили з упорядкованого ряду; розмах (R=x_{\max}-x_{\min}); коеф. варіації (v=s/\bar x).

31. Основні характеристики генеральної сукупності і способи їх обчислення.
Параметри (\mu, \sigma^2), розподіл (F(x)), квантили тощо; якщо відома вся сукупність — рахують «як є», якщо ні — оцінюють за вибіркою (точкові та інтервальні оцінки).

32. Дайте визначення поняттю «інтервальний варіаційний ряд».
Це групування значень змінної у вигляді інтервалів (класів) із частотами/відносними частотами для кожного інтервалу.

33. Що таке гістограма і з якою метою вона будується?
Гістограма — графік частот або щільностей по інтервалах; будується для наочного аналізу форми розподілу (симетрія, «хвости», мультимодальність, викиди).

34. Особливості критерію згоди  - Пірсона.
Перевіряє узгодженість емпіричних частот із теоретичними: (\chi^2=\sum\frac{(O_i-E_i)^2}{E_i}); потрібні достатні очікувані частоти (типово (E_i\ge 5)); число ступенів свободи зменшується на кількість оцінених параметрів. ([Wikipedia][1])

35. Особливості критерію згоди Романовського.
Використовують для малих (n) (зокрема для виявлення «промахів» у вимірюваннях): обчислюють (t=\left|\frac{x_i^*-\bar x}{\tilde\sigma}\right|) і порівнюють з табличним (t_{гр}(n,q)); якщо (t\ge t_{гр}), значення вважають промахом і відкидають. ([ela.kpi.ua][2])

36. Особливості критерію згоди Колмогорова.
Непараметричний критерій для неперервних розподілів: базується на максимальному відхиленні емпіричної функції розподілу від теоретичної (D=\sup_x|F_n(x)-F(x)|); не потребує групування в інтервали.

37. Послідовність обробки дослідних даних показовим законом.
Зібрати вибірку часів/інтервалів → оцінити параметр (\lambda) (часто (\hat\lambda=1/\bar t)) → побудувати теоретичну (F(t)=1-e^{-\lambda t}) → перевірити узгодженість (Колмогоров/Пірсон) → зробити висновок і оцінити похибки/довірчі інтервали.

38. Послідовність обробки дослідних даних нормальним законом.
Оцінити (\hat\mu=\bar x), (\hat\sigma=s) → побудувати теоретичну щільність/ФР → перевірити нормальність (К–С, Пірсон, Шапіро-Вілк) → проаналізувати залишки/викиди → зробити висновок.

39. Послідовність обробки дослідних даних законом Вейбулла.
Оцінити параметри форми (k) і масштабу (\lambda) (MLE або лінеаризація через (\ln(-\ln(1-F))) vs (\ln t)) → побудувати (F(t)=1-e^{-(t/\lambda)^k}) → перевірити узгодженість → інтерпретувати (характер відмов/інтенсивність).

40. Дайте визначення таким поняттям: випадковий процес; реалізація випадкового процесу; перетин випадкового процесу.
Випадковий процес (X(t)) — сімейство випадкових величин, індексованих часом/параметром (t). Реалізація (траєкторія) — конкретна функція (x(t)), що «вийшла» в одному експерименті. Перетин (зріз) — випадкова величина (X(t_0)) при фіксованому (t_0).

41. Наведіть класифікацію випадкових процесів.
За часом: дискретного/неперервного; за станами: дискретні/неперервні; стаціонарні/нестаціонарні; марковські/немарковські; гаусівські/негаусівські; з незалежними приростами/без них; процеси відновлення, Пуассона тощо.

42. Перерахуйте основні характеристики випадкових процесів.
Середня функція (m_X(t)=E[X(t)]), дисперсія (D[X(t)]), автокореляція/автоковаріація (R_X(t_1,t_2)=E[(X(t_1)-m(t_1))(X(t_2)-m(t_2))]), спектральні характеристики (для стаціонарних), розподіли приростів.

43. Дайте визначення потоку подій, назвіть ознаки, за якими вони поділяються.
Потік подій — випадковий набір моментів часу настання подій (або процес рахунку (N(t))). Поділяють за стаціонарністю, ординарністю (чи можливі «пачки»), наявністю післядії/незалежністю інтервалів, інтенсивністю (стала/змінна).

44. Перерахуйте властивості найпростішого потоку подій.
Найпростіший (пуассонівський) потік: стаціонарний, ординарний, має незалежні прирости; інтервали між подіями незалежні й мають показовий розподіл.

45. Що таке інтенсивність потоку подій? Фізичний зміст інтенсивності потоку подій.
Інтенсивність (\lambda) — середнє число подій за одиницю часу (границя (E[N(t+\Delta)-N(t)]/\Delta) при (\Delta\to0)); фізично це «швидкість надходження» подій/заявок.

46. Особливості потоку Пальма і Ерланга.
Потік Пальма — стаціонарний, ординарний потік з незалежними інтервалами між подіями (узагальнення найпростішого, але з «обмеженою післядією»). Потік Ерланга — спеціальний випадок (можна отримати, наприклад, «просіюванням» подій найпростішого потоку), де інтервал між подіями має ерлангівський розподіл порядку (k) (сума (k) експоненційних). ([web.posibnyky.vntu.edu.ua][3])

47. Дайте визначення Марковському випадковому процесу.
Марковський процес має властивість: умовний розподіл майбутнього стану за умови теперішнього не залежить від минулого (майбутнє залежить лише від поточного стану).

48. Назвіть основні характеристики випадкового процесу з дискретними станами і дискретним часом.
Початковий розподіл (p^{(0)}), матриця перехідних імовірностей (P=[p_{ij}]), (n)-крокові ймовірності (P^n), класи станів (поглинаючі/перехідні), стаціонарний розподіл (якщо існує).

49. Правила запису рівнянь Колмогорова.
Для неперервного часу (CTMC) з інтенсивностями (q_{ij}): прямі рівняння (\frac{dp_i(t)}{dt}=\sum_{j}p_j(t)q_{ji}) (або векторно (p'(t)=p(t)Q)); також існують зворотні рівняння.

50. Дайте визначення граничним ймовірностям станів.
Граничні ймовірності (\pi_i=\lim_{t\to\infty}p_i(t)) (або (\lim_{n\to\infty}p_i^{(n)}) у дискретному часі), якщо границя існує; це стаціонарний розподіл (за ергодичності).

51. Зобразіть графічно випадковий процес чистого «розмноження» і процес чистої «загибелі».
Чисте «розмноження» (pure birth): стани (0,1,2,\dots) і переходи лише (i\to i+1) з інтенсивностями (\lambda_i):
(0 \xrightarrow{\lambda_0} 1 \xrightarrow{\lambda_1} 2 \xrightarrow{\lambda_2} \dots)
Чиста «загибель» (pure death): переходи лише (i\to i-1) з (\mu_i):
(\dots 2 \xrightarrow{\mu_2} 1 \xrightarrow{\mu_1} 0).

52. Перерахуйте ознаки, за якими поділяються СМО.
За входом (A): закон надходжень; за обслуговуванням (S): закон часу обслуговування; число каналів (c); місткість системи (K); число джерел (N); дисципліна черги (FIFO/LIFO/пріоритети); наявність/відсутність очікування та відмов.

53. Назвіть основні вихідні параметри, які використовуються при аналізі роботи СМО.
(\lambda) — інтенсивність надходжень, (\mu) — інтенсивність обслуговування, (c) — число каналів, (K) — місткість, дисципліна, коефіцієнт завантаження (\rho), ефективна інтенсивність (\lambda_{еф}).

54. Запишіть основні імовірнісні показники функціонування СМО.
Ймовірності станів (p_n); ймовірність простою (p_0); ймовірність очікування (P_{wait}); ймовірність відмови (P_{loss}) (для систем з відмовами); пропускна здатність/вихідний потік (\lambda_{еф}).

55. Зобразіть розмічений граф станів багатоканальної СМО з очікуванням.
Типово стан (n) — число заявок у системі (в обслуговуванні+черзі), (n=0,1,\dots,K). Переходи: (n\to n+1) з інтенсивністю (\lambda) (поки (n<K)); (n\to n-1) з інтенсивністю (\min(n,c)\mu).

56. Запишіть формули підрахунку середнього числа зайнятих каналів і середнього числа заявок, що стоять в черзі.
Середнє число зайнятих каналів: (\bar m=\sum_{n=0}^{K}\min(n,c),p_n). Середня довжина черги: (L_q=\sum_{n=0}^{K}\max(n-c,0),p_n).

57. Основні поняття СМО
Заявка, вхідний потік, канал(и) обслуговування, черга, дисципліна обслуговування, час очікування, час обслуговування, відмова (втрата), пропускна здатність, завантаження.

58. Класифікація СМО
З відмовами/з очікуванням/з повторними викликами; одно- та багатоканальні; з обмеженою/необмеженою чергою; відкриті/закриті (скінченні джерела); з пріоритетами/без; стаціонарні/нестаціонарні.

59. Характеристики СМО
(L) — середнє число заявок у системі, (L_q) — у черзі, (W) — середній час у системі, (W_q) — очікування, (\rho) — завантаження, (P_{wait}), (P_{loss}), (\lambda_{еф}).

60. Моделі типових СМО
Класичні: M/M/1, M/M/c, M/M/1/K, M/M/c/K, M/G/1, G/M/1, Erlang B (втрати), Erlang C (очікування), моделі Енгсета (скінченні джерела), мережі Джексона.

62. Що таке система масового обслуговування?
СМО — система, в якій випадкові заявки надходять у часі, можуть очікувати в черзі та обслуговуються одним або кількома каналами за певними правилами.

63. З яких елементів складається система масового обслуговування?
Джерело/вхідний потік заявок, черга (місця очікування), канали обслуговування, дисципліна обслуговування, вихід (потік обслужених/втрачених).

64. Які економічні процеси описуються за допомогою систем масового обслуговування?
Черги в банках/касах, кол-центри, логістика/склади, ремонтні служби, виробничі лінії, транспортні потоки, комп’ютерні мережі та сервери, обробка замовлень.

65. Якими характеристиками оцінюється система масового обслуговування?
Час очікування/перебування, довжина черги/системи, завантаження ресурсів, ймовірність відмови/очікування, пропускна здатність, витрати.

66. Що таке потік подій системи масового обслуговування і як він описується?
Потік подій (вхідний потік заявок) — процес надходження заявок; описують (N(t)), інтервалами між надходженнями, інтенсивністю (\lambda(t)) і законом розподілу міжприходів (часто Пуассон/експоненційний).

67. Як описується процес обслуговування в СМО?  Формула Літтла.
Обслуговування описують законом часу обслуговування та інтенсивністю (\mu), а також числом каналів (c). Формула Літтла: (L=\lambda_{еф} W) і (L_q=\lambda_{еф} W_q).

68. Як описуються системи масового обслуговування в короткій формі?
Нотація Кендалла: (A/S/c/K/N/\text{Disc}) (часто скорочують до (A/S/c), якщо інше «за замовчуванням»).

69. Якими методами досліджуються системи масового обслуговування?
Аналітичні (Марковські моделі, рівняння Колмогорова, породжувальні функції), чисельні (розв’язання систем рівнянь), імітаційні (дискретно-подійне моделювання), наближені/асимптотичні.

70. Що таке граф станів і переходів системи масового обслуговування?
Це граф, де вершини — стани системи (напр., число заявок), а ребра — можливі переходи між станами (надходження/завершення обслуговування) з інтенсивностями/ймовірностями.

71. Які критерії ефективності використовуються в системах масового обслуговування?
Мінімізація середнього часу очікування/черги, мінімізація ймовірності втрат, максимізація пропускної здатності, баланс «вартість ресурсів + вартість очікування/втрат».

72. Які закони розподілу ймовірностей використовуються в системах масового обслуговування?
Пуассонівський потік (надходження), експоненційний/ерлангівський/гіперекспоненційний час обслуговування, а також загальні (G)-розподіли, нормальний/Вейбулла — у прикладних задачах.

73. У чому полягає сутність диференціального методу дослідження систем масового обслуговування?
У складанні та розв’язанні диференціальних рівнянь (Колмогорова) для ймовірностей станів (p_n(t)) у часі.

74. У чому полягає сутність методу фаз дослідження систем масового обслуговування?
Складний розподіл часу обслуговування представляють як послідовність «фаз» (phase-type, ерлангівські фази), розширюють стан і зводять задачу до марковської.

75. У чому полягає сутність методу включення додаткових змінних дослідження систем масового обслуговування?
Додають змінні, що «пам’ятають» потрібну історію (напр., залишковий час обслуговування, клас заявки), щоб зробити процес марковським і придатним до стандартного аналізу.

76. СМО з відмовами.
Це системи з обмеженою місткістю (без черги або з малою чергою), де заявка, що приходить у «повну» систему, втрачається (відмовляється), напр. M/M/c/c (Ерланг B).

77. У чому полягає особливість систем масового обслуговування з скінченною кількістю джерел запитів?
Інтенсивність надходжень залежить від числа «вільних» джерел: коли багато заявок уже в системі, менше джерел може генерувати нові; типова модель — Енгсет/закриті системи.

78. Назвіть базові показники для СМО з скінченною кількістю джерел запитів.
Ймовірності станів, середнє число зайнятих каналів, середнє число заявок у системі/черзі, середній час очікування/перебування, ефективна інтенсивність (продуктивність).

79. У чому полягає особливість систем масового обслуговування з пріоритетами?
Є кілька класів заявок, і правила обслуговування залежать від пріоритету (хто перший і чи можна переривати нижчий клас).

80. Назвіть базові види систем масового обслуговування з пріоритетами.
Неперервні (абсолютні, з витісненням — preemptive) і без витіснення (non-preemptive); статичні/динамічні пріоритети; детерміновані/ймовірнісні правила вибору.

81. Які виникають труднощі під час дослідження систем масового обслуговування з пріоритетами?
Зростає розмірність стану (потрібно рахувати окремо класи), ускладнюються залежності та формули показників, часто потрібні багатовимірні марковські ланцюги або апроксимації/імітація.

82. Зобразіть граф станів і переходів для СМО з абсолютним пріоритетом, двома вхідними потоками й двома місцями для очікування.
Зручно задавати стан як ((n_H,n_L)), де (n_H) — кількість високопріоритетних у системі, (n_L) — низькопріоритетних, з обмеженням (n_H+n_L\le 3) (1 в обслуговуванні + 2 в очікуванні). Переходи: прихід (H): ((n_H,n_L)\to(n_H+1,n_L)) з (\lambda_H) якщо місце є; прихід (L): ((n_H,n_L)\to(n_H,n_L+1)) з (\lambda_L) якщо місце є; завершення обслуговування з (\mu): зменшує (n_H) якщо (n_H>0), інакше зменшує (n_L).

83. Якими ознаками характеризуються мережі систем масового обслуговування?
Топологією (вузли/канали), правилами маршрутизації між вузлами, типами вузлів (A/S/c/…), наявністю класів заявок, відкритістю/закритістю мережі.

84. За яких умов мережа систем масового обслуговування функціонує в стаціонарному режимі?
Потрібна ергодичність/стійкість: у відкритій мережі — щоб навантаження в кожному вузлі було менше за пропускну здатність (умовно (\rho_i<1)); у закритій мережі зі скінченною популяцією стаціонарність зазвичай досяжна за коректних дисциплін.

85. Бібліотека SimPy, призначення, основні компоненти. Моделювання дискретних подій.
SimPy — Python-бібліотека для дискретно-подійного моделювання (черги, виробництво, логістика). Основні компоненти: Environment, Process, Event, Resource (і похідні), Timeout, Store/Container.

86. Поняття середовища в SimPy. Імітаційний контроль
Environment — «симуляційний годинник» і планувальник подій; контроль через `env.run(until=...)`, `env.now`, керування завершенням/зупинкою симуляції.

87. Події в SimPy. Операції з подіями, запуск подій, очікування кількох подій одночасно. Події та час SimPy
Event — об’єкт, що «спрацьовує» в майбутньому; процеси `yield` подію, щоб чекати. Для кількох подій використовують комбінації на кшталт `AnyOf/AllOf` (очікування першої або всіх). Час задають через `Timeout(dt)`.

88. Процеси в SimPy. Взаємодія процесів. Переривання процесів.
Process — генератор, що описує поведінку агента/системи в часі; взаємодіють через події та ресурси. Переривання (`interrupt`) дозволяє асинхронно змінювати хід процесу (напр., аварія, пріоритетна подія).

89. Ресурси в SimPy. Ресурси та переривання. Задання ресурсів, їх види, пріоритети.
Ресурси моделюють обмежені потужності (касир, верстат): `Resource`, `PriorityResource`, `PreemptiveResource`. Пріоритети визначають порядок доступу; preemption може переривати низький пріоритет.

90. Поняття контейнера, їх визначення. Stores, їх застосування та задання.
Container — ресурс-«запас» із рівнем (літри, деталі), операції `put/get`. Store — сховище дискретних об’єктів (черга предметів), з `put/get` і, за потреби, фільтрацією.

91. Моделювання в реальному часі в SimPy.
Використовують режим реального часу (RealtimeEnvironment), де симуляційний час прив’язують до «настінного» (wall-clock) часу для інтерактивних/онлайн-симуляцій.

92. Моніторинг процесу моделювання в SimPy. Відстеження подій та використання ресурсів.
Логують події з `env.now`, збирають статистику черг/ресурсів (довжина, зайнятість), застосовують callbacks/трасування, ведуть тайм-логи та післясимуляційну агрегацію показників.

93. Основні поняття і види часових рядів. Класифікація часових рядів.  
Часовий ряд — послідовність значень показника в часі. Класифікація: моментні/інтервальні; рівновіддалені/нерівновіддалені; стаціонарні/нестаціонарні; сезонні/несезонні; одно- та багатовимірні.

94. Вимоги порівнянності, однорідності, стійкості, достатньої сукупності спостережень. Коригування рівнів часових рядів. Метод Ірвіна.  
Порівнянність: одна методика вимірювання/ціни/одиниці; однорідність: без зміни змісту показника; стійкість: без різких структурних зламів без пояснення; достатність: достатня довжина для висновків. Коригування: приведення до спільної бази (дефляція, перерахунок, усунення зміни методики). Метод Ірвіна для викидів: обчислюють (\lambda_t=\frac{|y_t-y_{t-1}|}{\sigma_y}) і порівнюють з табличним (\lambda_\alpha); якщо більше — рівень підозрілий (аномальний). ([StudFiles][4])

95. Стацiонарнiсть часових рядiв. Функцiя автокореляцiї. Функція правдоподiбностi  
Стаціонарність (у слабкому сенсі): сталий середній рівень і ковариація залежить лише від лаґу. ACF (r(k)) показує кореляцію між (y_t) і (y_{t-k}). Функція правдоподібності (L(\theta)=\prod f(y_t|\theta)) використовується для оцінювання параметрів (MLE).

96. Розрахунок характеристик динаміки розвитку економічних процесів.  
Обчислюють абсолютний приріст (\Delta y_t=y_t-y_{t-1}), темп зростання (T_t=y_t/y_{t-1}), темп приросту (t_t=T_t-1), середні показники (середній рівень, середній приріст, середній темп).

97. Статистичні характеристики часових рядів.  
Середній рівень, дисперсія/варіація, коеф. варіації, автокореляції, сезонні індекси, тренд (параметри), асиметрія/ексцес, характеристики залишків.

98. Структурний аналіз часового ряду (тренд, циклічна, сезонна, випадкова складові). Різниця між сезонними та циклічними складовими часового ряду.  
Ряд подають як (Y=T+C+S+E) (або мультиплікативно). Сезонність — регулярні коливання з фіксованим періодом (напр., 12 міс.), цикл — довші хвилі зі змінною тривалістю (бізнес-цикл).

99. Стаціонарні та нестаціонарні процеси.  
Стаціонарні мають незмінні характеристики в часі; нестаціонарні — тренд/зміни дисперсії/структурні злами, через що стандартні моделі потребують диференціювання або перетворень.

100. Білий шум.
Білий шум — послідовність з нульовим середнім, сталою дисперсією та відсутністю автокореляції (для (k\ne0): (r(k)=0)).

101. Ідентифікація часового ряду.
Це вибір адекватної моделі (тип, порядок, сезонність) і оцінка її параметрів за даними (напр., підбір ARIMA(p,d,q) за ACF/PACF та критеріями).

102. Перевірка стаціонарності та визначення порядку інтегрування (метод Форстера-Стьюарта, метод Діккі-Фуллера).
ADF (Діккі–Фуллер) перевіряє наявність одиничного кореня: якщо не відхиляємо (H_0) — ряд нестаціонарний і потрібне диференціювання; порядок інтегрування (d) — мінімальна кількість різниць для стаціонарності. Метод Форстера–Стьюарта — тест на наявність тренду/зміни середнього через аналіз послідовних максимумів/мінімумів та статистик для «зростання/спаду».

103. Прогнозування за середніми характеристиками ряду: екстраполяція на основі середнього рівня ряду, екстраполяція за середнім абсолютним приростом, екстраполяція за середнім темпом зростання.
За середнім рівнем: (\hat y_{t+h}=\bar y). За середнім приростом: (\hat y_{t+h}=y_t+h\overline{\Delta}). За середнім темпом: (\hat y_{t+h}=y_t(\overline{T})^h).

104. Аналітичні методи згладжування часових рядів.
Згладжування через аналітичну апроксимацію тренду (МНК: лінійний/поліноміальний/експоненційний), експоненційне згладжування, регресійні трендові моделі.

105. Екстраполяція трендів на основі кривих зростання.
Будують криву зростання (експоненціальна, логістична, Гомперця тощо), оцінюють параметри за історією і продовжують тренд на майбутні моменти часу.

106. Вибір функції, що характеризує тенденцію: лінійна, експоненціальна, степенева, гіперболічна, логістична, функція Гомперця тощо. Зведення кривої зростання до лінійної регресії.
Функцію обирають за формою динаміки та якістю апроксимації (помилки, (R^2), залишки). Лінеаризація робиться перетвореннями (напр., (\ln y), (1/y), (\ln\frac{y}{K-y})) щоб отримати лінійну регресію параметрів.

107. Метод ковзного середнього. На які 2 види поділяють метод ковзного середнього? Яка між ними різниця?
Просте ковзне середнє (SMA) — однакові ваги в вікні; зважене ковзне середнє (WMA) — різні ваги (часто більші для останніх спостережень), тому реагує швидше.

108. Порядок аналiзу часових рядiв. Розбиття часових рядiв. Пропорцiйне розбиття
Зазвичай: попередній аналіз → очищення/коригування → перевірка стаціонарності → моделювання → діагностика → прогноз → верифікація. Розбиття: train/validation/test у часовому порядку; пропорційне — фіксовані частки (напр., 70/15/15) без перемішування.

109. Метод центрованих та зважених ковзних середніх.
Центроване ковзне середнє вирівнює значення так, щоб середнє відносилось до «центру» вікна (корисно для сезонності). Зважене — використовує ваги (w_i) у вікні: (\hat y_t=\sum w_i y_{t-i}), (\sum w_i=1).

110. Mетод простого експоненціального згладжування. Вибір параметрів згладжування. Розрахунок прогнозу.
(S_t=\alpha y_t+(1-\alpha)S_{t-1}), (0<\alpha<1); прогноз: (\hat y_{t+1}=S_t). (\alpha) підбирають мінімізуючи SSE/MSE на навчальній частині або за критеріями.

111. Методи усереднення (moving average)
SMA, WMA, центроване MA, експоненціальне MA (EMA), сезонні ковзні середні (по періоду).

112. Характеристики динаміки часового ряду. Коригування рівнів часового ряду
Характеристики: (\Delta y_t), (T_t), середні прирости/темпи, індекси. Коригування: приведення до порівнянної бази, усунення структурних змін (перерахунок), дефляція, усунення викидів/аномалій.

113. Адаптивні методи прогнозування: Брауна, Хольта.
Брауна (подвійне експоненційне) враховує тренд через дві послідовні згладжені серії; Холта — два рівняння: рівень (L_t=\alpha y_t+(1-\alpha)(L_{t-1}+B_{t-1})), тренд (B_t=\beta(L_t-L_{t-1})+(1-\beta)B_{t-1}), прогноз (h)-кроків: (\hat y_{t+h}=L_t+hB_t).

114. Методи фільтрації сезонної компоненти.
Декомпозиція (адитивна/мультиплікативна), сезонні індекси, сезонні ковзні середні, регресія з сезонними дамі-змінними, X-11/SEATS-подібні ітераційні процедури.

115. Фільтрація сезонної компоненти за допомогою індексу сезонності.
Обчислюють сезонні індекси (S_j) для кожної позиції сезону (місяць/квартал) і «знімають» сезонність: адитивно (y_t^{(ds)}=y_t-S_j) або мультиплікативно (y_t^{(ds)}=y_t/S_j).

116. Метод декомпозиції часового ряду.
Розкладає ряд на компоненти: адитивно (Y=T+S+C+E) або мультиплікативно (Y=T\cdot S\cdot C\cdot E), оцінюючи їх послідовним згладжуванням/усередненням.

117. Ітераційні методи фільтрації.
Повторюють оцінку тренду й сезонності по колу (оновлюючи індекси та залишки) до стабілізації; приклад — X-11-подібні схеми сезонного коригування.

118. Моделі прогнозування сезонних процесів.
SARIMA, Holt–Winters (трипараметричне згладжування), сезонна регресія з дамі, STL + модель для залишків, сезонні state-space моделі.

119. Множинна регресія.
Регресія, де (y) залежить від кількох факторів: (y=\beta_0+\sum_{j=1}^k\beta_j x_j+\varepsilon) (або нелінійні узагальнення).

120. Прогнозування на основі багатофакторних регресійних моделей.
Оцінюють модель на історичних даних → перевіряють значущість/якість → підставляють прогнозні або сценарні значення факторів (x_j) → отримують прогноз (\hat y) та інтервал прогнозування.

121. Індивідуальні експертні методи: метод інтерв’ю, аналітичні експертні оцінки.
Інтерв’ю — структуроване отримання суджень експерта. Аналітичні оцінки — експерт самостійно формує прогноз/оцінку, обґрунтовуючи логікою, даними та досвідом.

122. Колективні експертні методи: метод комісій, колективної генерації ідей («мозкової атаки»), дельфійський метод.
Комісія — спільне обговорення й узгодження. Мозкова атака — генерація максимальної кількості ідей без критики з подальшим відбором. Дельфі — анонімні раунди опитувань із зворотним зв’язком до зближення думок.

123. Методика розрахунку експертних оцінок.
Збирають оцінки → нормалізують (приводять до шкали) → зважують за компетентністю → агрегують (середнє/медіана/рангові методи) → перевіряють узгодженість (коеф. конкордації тощо).

124. Етапи проведення експертизи.
Постановка мети → добір експертів → підготовка анкети/шкали → збір оцінок (раунди) → агрегація/узгодження → аналіз надійності → оформлення висновку.

125. Вибір експертів та визначення їх чисельності.
Обирають за компетентністю, досвідом, незалежністю; чисельність — достатня для стабільної агрегації (часто 5–15+ залежно від задачі), з урахуванням витрат і необхідної точності.

126. Оцінка відносної важливості та методика її розробки.
Визначають ваги критеріїв/факторів через ранжування, парні порівняння (AHP), бальні шкали; далі нормують ваги так, щоб сума дорівнювала 1.

127. Оцінка питомої ваги різних видів рішень.
Перетворюють експертні бали/ранги у частки: (w_i=\frac{s_i}{\sum s_i}) (або через частоти вибору), отримуючи «питому вагу» кожного рішення.

128. Перевірка адекватності моделі прогнозування.
Перевіряють відповідність даним: аналіз залишків, значущість параметрів, (R^2), F-тест, помилки на тесті (out-of-sample), відсутність систематичних відхилень.

129. Перевірка випадковості коливань рівнів залишкової послідовності.
Застосовують тести випадковості (runs test), тест поворотних точок, аналіз ACF залишків; випадковість означає відсутність структури/серіалізації.

130. Перевірка відповідності випадкової компоненти нормальному закону розподілу.
Q–Q графік, Shapiro–Wilk, Jarque–Bera, Kolmogorov–Smirnov, (\chi^2)-узгодженість — залежно від припущень і обсягу вибірки.

131. Перевірка рівності математичного сподівання випадкової компоненти нулю.
Тестують (H_0:E=0): t-тест для середнього залишків (або довірчий інтервал для (\bar\varepsilon)).

132. Перевірка незалежності значень випадкової компоненти.
Перевіряють автокореляцію залишків (ACF), Durbin–Watson (для AR(1)-подібної), Ljung–Box (для багатьох лаґів).

133. Перевірка точності моделі прогнозування.
Використовують RMSE, MAE, MAPE, sMAPE, Theil’s U, порівнюють із наївним прогнозом, аналізують помилки на відкладеній вибірці.

134. Основні статистики: коефіцієнти детермінації та кореляції, F- критерій, побудова довірчих інтервалів параметрів за допомогою t- критерію.
(R^2=1-\frac{SSE}{SST}), (r) — кореляція; F-тест перевіряє загальну значущість регресії; t-критерій дає ДІ для (\beta_j): (\beta_j\pm t_{\alpha/2,df}\cdot SE(\beta_j)).

135. Прогнозування за трендовою моделлю. Основні статистики міри точності прогнозів. Побудова інтервалів прогнозування.
Прогнозують (\hat y_{t+h}) з трендової функції; точність — MAE/RMSE/MAPE. Інтервал прогнозування: (\hat y \pm t\cdot SE_{pred}) (де (SE_{pred}) враховує невизначеність параметрів і випадкову похибку).

136. Перевірка прогнозної якості моделі.
Оцінюють помилки на тестовому періоді, стабільність параметрів, порівняння з альтернативами (наївна, інша модель), перевіряють, чи немає систематичного зсуву.

137. Критерії визначення якісного прогнозу. Прогноз раціональних сподівань.
Якісний прогноз має малу похибку, без систематичного зсуву, стабільний у часі та кращий за базову модель. Раціональні сподівання: прогноз використовує всю доступну інформацію так, що помилка не прогнозована на основі відомих даних (умовно «несподівана»).

138. Стандартні критерії ефективності та незміщеності.
Ефективність — мінімальна дисперсія помилки за заданих умов (часто MSE). Незміщеність — (E[\hat y - y]=0) (середня помилка дорівнює нулю).

139. Параметричні та непараметричні показники точності прогнозу.
Параметричні спираються на припущення про розподіл/модель (напр., інтервали через нормальність, критерії t/F). Непараметричні не вимагають розподільчих припущень і базуються на самих похибках (MAE, MdAE, MAPE, рангові/знакові оцінки).
